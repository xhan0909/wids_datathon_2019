{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import json\n",
    "import scipy\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(str(PATH/'traininglabels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(train_labels['has_oilpalm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(train_labels['has_oilpalm'])/len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path):\n",
    "    img = cv2.imread(str(img_path)).astype(np.float32)/255\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img_rgb\n",
    "\n",
    "def show_image(paths, nrow=2, ncol=3):\n",
    "    n = nrow * ncol\n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=(12, 8))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = load_image(paths[i][0])\n",
    "        ax.imshow(img)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if paths[i][1] == 0:\n",
    "            ax.set_title(f'No oil palm trees, label score is {paths[i][2]}')\n",
    "        else:\n",
    "            ax.set_title(f'Has oil palm trees, label score is {paths[i][2]}')\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names = random.sample(list(train_labels['image_id']), 6)\n",
    "img_labels = [train_labels.loc[train_labels['image_id'] == p, 'has_oilpalm'].values[0] for p in img_names]\n",
    "label_score = [train_labels.loc[train_labels['image_id'] == p, 'score'].values[0] for p in img_names]\n",
    "\n",
    "img_paths = [(str(PATH/f\"train_images/{p}\"), l, s) for p, l, s in zip(img_names, img_labels, label_score)]\n",
    "img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all images are 256x256\n",
    "img_size = 128\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(im):\n",
    "    \"\"\"Normalizes images with Imagenet stats.\"\"\"\n",
    "    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "    return (im - imagenet_stats[0])/imagenet_stats[1]\n",
    "\n",
    "def denormalize(im):\n",
    "    \"\"\"Denormalizes images.\"\"\"\n",
    "    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "    return (im * imagenet_stats[1]) + imagenet_stats[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_cv(im, deg, mode=cv2.BORDER_REFLECT, interpolation=cv2.INTER_AREA):\n",
    "    \"\"\" Rotates an image by deg degrees\"\"\"\n",
    "    r,c,*_ = im.shape\n",
    "    M = cv2.getRotationMatrix2D((c/2,r/2),deg,1)\n",
    "    return cv2.warpAffine(im,M,(c,r), borderMode=mode, flags=cv2.WARP_FILL_OUTLIERS+interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oilPalmDataset(Dataset):\n",
    "    def __init__(self, dataset, size, transform=False, test=False):\n",
    "        self.dataset = dataset[:, 0]\n",
    "        self.test = test\n",
    "        self.transforms = transform\n",
    "        self.sz = size\n",
    "        if not self.test:\n",
    "            self.y = dataset[:, 1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path = self.dataset[i]\n",
    "        X = load_image(path)\n",
    "        if not self.test:\n",
    "            X = cv2.resize(X, (self.sz, self.sz))\n",
    "        if self.transforms:\n",
    "            random_degree = (np.random.random()-.50)*10 # +5/-5 degrees\n",
    "            X = rotate_cv(X, random_degree)\n",
    "            if np.random.random() > 0.5: # .5 prob random flip\n",
    "                X = np.fliplr(X).copy()\n",
    "        X = normalize(X)\n",
    "        X = X.transpose(2, 0, 1)\n",
    "        if not self.test:\n",
    "            y = int(self.y[i])\n",
    "            return X, y\n",
    "        else:\n",
    "            return X, str(path).split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(train_labels.loc[:, 'image_id'])\n",
    "X = [str(PATH/f\"train_images/{p}\") for p in X]\n",
    "y = list(train_labels.loc[:, 'has_oilpalm'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training set is unbalanced, let's check if the train/valid dataset is stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_train)/(len(y)*.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_val)/(len(y)*.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.concatenate([np.array(X_train).reshape(-1, 1), np.array(y_train).reshape(-1, 1)], axis=1)\n",
    "valid = np.concatenate([np.array(X_val).reshape(-1, 1), np.array(y_val).reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = oilPalmDataset(train, size=img_size, transform=True)\n",
    "valid = oilPalmDataset(valid, size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_dl))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        layers = list(resnet.children())[:-2]\n",
    "        self.top_model = nn.Sequential(*layers).cuda()\n",
    "        self.out = 2048\n",
    "        self.bn1 = nn.BatchNorm1d(self.out)\n",
    "        self.bn2 = nn.BatchNorm1d(2048)\n",
    "        self.fc1 = nn.Linear(self.out, 2048)\n",
    "        self.fc2 = nn.Linear(2048, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.top_model(x))\n",
    "        x = nn.AdaptiveAvgPool2d((1,1))(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = nn.Dropout(0.2)(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = nn.Dropout(0.2)(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr = 0.01, wd = 0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim\n",
    "\n",
    "\n",
    "def LR_range_finder(model, train_dl, lr_low=1e-6, lr_high=1, epochs=2):\n",
    "    losses = []\n",
    "    iterations = epochs * len(train_dl)\n",
    "    delta = (lr_high - lr_low)/iterations\n",
    "    lrs = [lr_low + i*delta for i in range(iterations)]\n",
    "    model.train()\n",
    "    ind = 0\n",
    "    for i in range(epochs):\n",
    "        for x, y in train_dl:\n",
    "            optim = get_optimizer(model, lr=lrs[ind])\n",
    "            x = x.cuda().float()\n",
    "            y = y.cuda()\n",
    "            out = model(x)\n",
    "            loss = F.cross_entropy(out, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            losses.append(loss.item())\n",
    "            ind +=1\n",
    "\n",
    "    return lrs, losses \n",
    "\n",
    "\n",
    "def save_model(m, p): \n",
    "    torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): \n",
    "    m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().cuda()\n",
    "lrs, losses = LR_range_finder(model, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "plt.plot(lrs[:100], losses[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triangular_lr2(lr_low, lr_high, stepesize):\n",
    "    iterations = 2*stepesize\n",
    "    iter1 = int(0.35*iterations)\n",
    "    iter2 = int(0.85*iter1)\n",
    "    iter3 = iterations - iter1 - iter2\n",
    "    delta1 = (lr_high - lr_low)/iter1\n",
    "    delta2 = (lr_high - lr_low)/(iter1 -1)\n",
    "    lrs1 = [lr_low + i*delta1 for i in range(iter1)]\n",
    "    lrs2 = [lr_high - i*(delta1) for i in range(0, iter2)]\n",
    "    delta2 = (lrs2[-1] - lr_low)/(iter3)\n",
    "    lrs3 = [lrs2[-1] - i*(delta2) for i in range(1, iter3+1)]\n",
    "    return lrs1+lrs2+lrs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triangular_policy(model, train_dl, valid_dl, lr_low=1e-6, lr_high=0.1):\n",
    "    idx = 0\n",
    "    epochs = 4\n",
    "    stepesize = 2*len(train_dl)\n",
    "    lrs = get_triangular_lr2(lr_low, lr_high, stepesize)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        for i, (x, y) in enumerate(train_dl):\n",
    "            optim = get_optimizer(model, lr = lrs[idx], wd =0)\n",
    "            batch = y.shape[0]\n",
    "            x = x.cuda().float()\n",
    "            y = y.cuda()\n",
    "            out = model(x)\n",
    "            loss = F.cross_entropy(out, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            idx += 1\n",
    "            total += batch\n",
    "            sum_loss += batch*(loss.item())\n",
    "        print(\"train loss %.3f\" % (sum_loss/total))\n",
    "        val_metrics(model, valid_dl)\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0 \n",
    "    for i, (x, y) in enumerate(valid_dl):\n",
    "        batch = y.shape[0]\n",
    "        x = x.cuda().float()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "#         _, pred = torch.max(out, 1)\n",
    "#         correct += pred.eq(y).sum().item()\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += batch*(loss.item())\n",
    "        total += batch\n",
    "    print(\"val loss %.3f and accuracy score %.3f\" % (sum_loss/total, correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def training_loop(model, train_dl, valid_dl, steps=3, lr_low=1e-6, lr_high=0.1):\n",
    "    for i in range(steps):\n",
    "        start = datetime.now() \n",
    "        loss = train_triangular_policy(model, train_dl, valid_dl, lr_low, lr_high)\n",
    "        end = datetime.now()\n",
    "        t = 'Time elapsed {}'.format(end - start)\n",
    "        print(\"----End of step\", t, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before training\n",
    "val_metrics(model, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(model, train_dl, valid_dl, steps=1, lr_low=1e-6, lr_high=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"./baseline-model-98.pth\"\n",
    "save_model(model, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze(7)\n",
    "model.unfreeze(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check validation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"./baseline-model-98.pth\"\n",
    "model = Net().cuda()\n",
    "model.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_out = []\n",
    "\n",
    "model.eval()\n",
    "for X, y in valid_dl:\n",
    "    X = X.float().cuda()\n",
    "    y_hat = model(X)\n",
    "    _, pred = torch.max(y_hat, 1)\n",
    "    val_out.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = []\n",
    "val_y = []\n",
    "for X, y in valid_dl:\n",
    "    val_X.append(X)\n",
    "    val_y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_out = [t.cpu().numpy() for t in val_out]\n",
    "val_out = np.concatenate(val_out).ravel()\n",
    "val_y = np.concatenate(val_y).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y.shape, val_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_idx = np.where(~np.equal(val_out, val_y))[0]\n",
    "diff_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in diff_idx:\n",
    "    print(f'Truth: {val_y[idx]}, prediction: {val_out[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH_1 = PATH/'leaderboard_test_data'\n",
    "TEST_PATH_2 = PATH/'leaderboard_holdout_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_path_1 = np.array(list(TEST_PATH_1.iterdir())).reshape(-1, 1)\n",
    "X_test_path_2 = np.array(list(TEST_PATH_2.iterdir())).reshape(-1, 1)\n",
    "X_test_path = np.concatenate([X_test_path_1, X_test_path_2])\n",
    "X_test_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = oilPalmDataset(X_test_path, size=256, test=True)\n",
    "test_dl = DataLoader(test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, paths = next(iter(test))\n",
    "X.shape, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "p = \"./baseline-model-98.pth\"\n",
    "model = Net().cuda()\n",
    "model.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "fpaths = []\n",
    "\n",
    "model.eval()\n",
    "for X, path in test_dl:\n",
    "    X = X.float().cuda()\n",
    "    y_hat = model(X)\n",
    "    pred = y_hat[:, 1]\n",
    "    out.append(pred)\n",
    "    fpaths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [t.detach().cpu().numpy() for t in out]\n",
    "out = np.concatenate(out).ravel()\n",
    "fpaths = np.concatenate(fpaths).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out = sorted(list(zip(fpaths, out)), key=lambda x: x[0])\n",
    "\n",
    "with open('test_output_pytorch.csv','w') as f:\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(['image_id', 'has_oilpalm'])\n",
    "    for row in pred_out:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "381px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "313px",
    "left": "1518px",
    "right": "20px",
    "top": "120px",
    "width": "367px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
